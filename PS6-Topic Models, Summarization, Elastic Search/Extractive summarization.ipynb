{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pyrouge import Rouge155\n",
    "stopset=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Newsgroups in elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(['localhost'],\n",
    "    http_auth=('elastic', 'elastic'),\n",
    "    scheme=\"http\",\n",
    "    port=9200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ng = fetch_20newsgroups(subset='all',remove=('headers'))\n",
    "ng_X = ng.data\n",
    "\n",
    "ngdocs={}\n",
    "for i in range(len(ng_X)):\n",
    "    ngdocs[i]=ng_X[i].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = fetch_20newsgroups(subset='all',remove=('headers','footers'))\n",
    "#ng_X=([re.sub(\"[^a-zA-Z ]\",\"\",\" \".join(k.replace('\\n','').lower() for k in i.split())) for i in ng.data])\n",
    "ng_X=ng.data\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=3,max_features=10000,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(ng_X)\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=3,max_features=10000,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(ng_X)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=20, max_iter=50, \n",
    "      learning_method='online', learning_offset=50., random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es.indices.refresh(index=\"newsgroups-index\")\n",
    "\n",
    "res = es.search(index=\"newsgroups-index\", body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klscores=[]\n",
    "ldascores=[]\n",
    "\n",
    "for i in range(len(ng_X)):\n",
    "\n",
    "    origdoc = \" \".join([j for j in ng_X[i].split()])\n",
    "\n",
    "    klsummary = \" \".join(kl_summarization(sent_tokenize(origdoc),len(sent_tokenize(origdoc))*0.5))\n",
    "    ldasummary = \" \".join(topic_summarization(sent_tokenize(origdoc),len(sent_tokenize(origdoc))*0.5))\n",
    "    \n",
    "    es.update(index=\"newsgroups-index\", doc_type='newsgroups', id=i, \n",
    "      body={\"doc\": {'doc_id': i,'LDA_Summary': ldasummary, 'KL_Summary':klsummary}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing DUC dataset in elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ducpath=\"/Users/sasankauppu/Desktop/Data Mining CS6220/DataMining/DUC2001/\"\n",
    "ducdocs={}\n",
    "ducsum={}\n",
    "for f in os.listdir(ducpath+\"raw_data\"):\n",
    "    if f!=\".DS_Store\":\n",
    "        ducdocs[f.lower()]=BeautifulSoup(open(ducpath+'raw_data/'+f,'r').read(), \"lxml\").find(\"text\").text.replace('\\n','')\n",
    "    \n",
    "for f in os.listdir(ducpath+\"Summaries\"):\n",
    "    if f!=\".DS_Store\":\n",
    "        ducsum[f[:-4].lower()]=open(ducpath+'Summaries/'+f,'r').read().replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "duc_X = ducdocs.values()\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "tfidf_vectorizer_duc = TfidfVectorizer(max_df=0.5, min_df=3,stop_words='english')\n",
    "tfidf_duc = tfidf_vectorizer_duc.fit_transform(duc_X)\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "tf_vectorizer_duc = CountVectorizer(max_df=0.5, min_df=3,stop_words='english')\n",
    "tf_duc = tf_vectorizer_duc.fit_transform(duc_X)\n",
    "\n",
    "lda_duc = LatentDirichletAllocation(n_components=20, max_iter=50, \n",
    "      learning_method='online', learning_offset=50., random_state=0).fit(tf_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "klscores=[]\n",
    "ldascores=[]\n",
    "\n",
    "for i in ducdocs:\n",
    "\n",
    "    origdoc = \" \".join([j for j in ducdocs[i].split()])\n",
    "\n",
    "    klsummary = \" \".join(kl_summarization(sent_tokenize(origdoc),len(sent_tokenize(origdoc))*0.5))\n",
    "    ldasummary = \" \".join(topic_summarization(sent_tokenize(origdoc),len(sent_tokenize(origdoc))*0.5))\n",
    "    \n",
    "    if(i in ducsum):\n",
    "        originalsum = \" \".join([j for j in ducsum[i].split()])\n",
    "\n",
    "        klscores.append(rouge_n(sent_tokenize(klsummary),sent_tokenize(originalsum)))\n",
    "        ldascores.append(rouge_n(sent_tokenize(ldasummary),sent_tokenize(originalsum))\n",
    "\n",
    "    es.update(index=\"duc-index\", doc_type='ducdocs', id=i, \n",
    "      body={\"doc\": {'doc_id': i,'LDA_Summary': ldasummary, 'KL_Summary':klsummary}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Precision:  0.967942879516 Recall:  0.398614247811 FScore:  0.558958968113\n",
      "LDA Precision:  0.970624265337 Recall:  0.502696877352 FScore:  0.653761659023\n"
     ]
    }
   ],
   "source": [
    "kltot=[sum(x) for x in zip(*klscores)]\n",
    "ldatot=[sum(x) for x in zip(*ldascores)]\n",
    "\n",
    "print \"KL Precision: \",kltot[0]/len(klscores),\"Recall: \",kltot[1]/len(klscores),\"FScore: \",kltot[2]/len(klscores)\n",
    "print \"LDA Precision: \",ldatot[0]/len(ldascores),\"Recall: \",ldatot[1]/len(ldascores),\"FScore: \",ldatot[2]/len(ldascores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merged_freq(wlist1, wlist2):\n",
    "    wc1 = compute_word_freq(wlist1)\n",
    "    wc2 = compute_word_freq(wlist2)\n",
    "    merged = wc1.copy()\n",
    "\n",
    "    for k in wc2:\n",
    "        if k in merged: \n",
    "            merged[k] += wc2[k]\n",
    "        else:\n",
    "            merged[k] = wc2[k]\n",
    "\n",
    "    for k in merged:\n",
    "        merged[k] /= float(len(wlist1) + len(wlist2))\n",
    "\n",
    "    return merged\n",
    "    \n",
    "def compute_word_freq(wlist1):\n",
    "    word_freq = {}\n",
    "    for w in wlist1:\n",
    "        word_freq[w] = word_freq.get(w, 0) + 1\n",
    "    return word_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(summary_freq, doc_freq):\n",
    "    sum_val = 0\n",
    "    for w in summary_freq:\n",
    "        frequency = doc_freq.get(w)\n",
    "        if frequency:\n",
    "            sum_val += frequency * math.log(frequency / summary_freq[w])\n",
    "    return sum_val\n",
    "\n",
    "\n",
    "def kl_summarization(sentences,summary_length):\n",
    "    word_freq = [w.lower() for s in sentences for w in s.split()]\n",
    "    wcount = len(word_freq)\n",
    "    word_freq = compute_word_freq(word_freq)\n",
    "    word_freq = dict((w, float(f) / wcount) for w, f in word_freq.items())\n",
    "    \n",
    "    klsummary = {}\n",
    "    summary = []\n",
    "    vocab = [[w.lower() for w in s.split()] for s in sentences]\n",
    "    origsentences = sentences[:]\n",
    "\n",
    "    while len(sentences) > 0 and len(klsummary)<=summary_length:\n",
    "        kls = []\n",
    "        summarysplit = [w for s in summary for w in s.split()]\n",
    "\n",
    "        for s in vocab:\n",
    "            joint_freq = merged_freq(s, summarysplit)\n",
    "            kls.append(kl_divergence(joint_freq, word_freq))\n",
    "\n",
    "        ind = kls.index(min(kls))\n",
    "        top_sentence = sentences.pop(ind)\n",
    "        del vocab[ind]\n",
    "        summary.append(top_sentence)\n",
    "\n",
    "        klsummary[top_sentence] =  origsentences.index(top_sentence)\n",
    "    \n",
    "\n",
    "    return sorted(klsummary, key=klsummary.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_lists(summary_freq, doc_freq):\n",
    "    sum_val = 0\n",
    "    for w in range(len(summary_freq)):\n",
    "        frequency = doc_freq[w]\n",
    "        sum_val += frequency * math.log(frequency / summary_freq[w])\n",
    "    return sum_val\n",
    "\n",
    "def topic_summarization(sentences,summary_length):\n",
    "    word_freq = lda.transform(tfidf_vectorizer.transform([\"\".join(sentences)]))\n",
    "    \n",
    "    klsummary = {}\n",
    "    summary = []\n",
    "    vocab = [s for s in sentences]\n",
    "    origsentences = sentences[:]\n",
    "\n",
    "    while len(sentences) > 0 and len(klsummary)<=summary_length:\n",
    "        kls = []\n",
    "\n",
    "        for s in vocab:\n",
    "            joint_freq = lda.transform(tfidf_vectorizer.transform([\"\".join(summary)+s]))\n",
    "            \n",
    "            kls.append(kl_divergence_lists(joint_freq[0], word_freq[0]))\n",
    "            \n",
    "        ind = kls.index(min(kls))\n",
    "        top_sentence = sentences.pop(ind)\n",
    "        del vocab[ind]\n",
    "        summary.append(top_sentence)\n",
    "\n",
    "        klsummary[top_sentence] =  origsentences.index(top_sentence)\n",
    "    \n",
    "\n",
    "    return sorted(klsummary, key=klsummary.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def generate_ngrams(n, text):\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "\n",
    "def ngrams(n, sentences):\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        sentence = \" \".join([w for w in sentence.split() if not w in stopset])\n",
    "        words.update(generate_ngrams(n, sentence.split()))\n",
    "        \n",
    "    return words\n",
    "\n",
    "\n",
    "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n",
    "    \n",
    "    evaluated_ngrams = ngrams(n, evaluated_sentences)\n",
    "    reference_ngrams = ngrams(n, reference_sentences)\n",
    "    \n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "\n",
    "    precision = float(len(overlapping_ngrams)) / len(evaluated_ngrams)\n",
    "    recall = float(len(overlapping_ngrams)) / len(reference_ngrams)\n",
    "    fscore = (2*precision*recall)/(precision+recall)\n",
    "    \n",
    "    return (precision,recall,fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
